\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}dd}{\footnotesize  1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}dd}{\footnotesize  3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}dd}{\footnotesize  5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}dd}{\footnotesize  7}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}dd}{\footnotesize  9}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}dd}{\footnotesize  11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Matrices and Linear Systems}{\footnotesize  13}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}dd}{\footnotesize  13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}dd}{\footnotesize  13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}dd}{\footnotesize  13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}dd}{\footnotesize  13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Gaussian Elimination, Pivoting, and LU Factorization}{\footnotesize  13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PROGRAM 7.5:}{\footnotesize  14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EXAMPLE 7.15:}{\footnotesize  18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EXAMPLE 7.16:}{\footnotesize  20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EXERCISES 7.5:}{\footnotesize  20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}VECTOR AND MATRIX NORMS, ERROR ANALYSIS, AND EIGENDATA}{\footnotesize  24}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Graphic for the matrix norm definition (38). The matrix $A$ will transform $x$ into another vector Ax (of same dimension if $A$ is square). The norm of $A$ is the maximum magnification that the transformed vector $Ax$ norm will have in terms of the norm of $x$.\relax }}{\footnotesize  26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Heuristic diagram showing the distance from a nonsingular matrix A to the set of all singular matrices (line). \relax }}{\footnotesize  27}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Heuristic illustration showing the unreliability of the residual as a gauge to measure the error. This phenomenon is a special case of the general principle that a function having a very small derivative can have very close outputs resulting from different inputs spread far apart. \relax }}{\footnotesize  29}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Actions of the matrix $A=\left [\begin  {array}{cc}-2 & -1   0 & 1\end  {array}\right ]$ on a pair of vectors: (a) (left) The (shorter) vector $x=\left [\begin  {array}{l}1   1\end  {array}\right ]$ of length $\sqrt  {2}$ gets transformed to the vector $y=A x=\left [\begin  {array}{c}-3   1\end  {array}\right ]$ of length $\sqrt  {10}$. Since the two vectors are not parallel, $x$ is not an eigenvector of $A$. (b) (right) The (shorter) unit vector $x=\left [\begin  {array}{l}1   0\end  {array}\right ]$ gets transformed to the vector $y=A x=\left [\begin  {array}{c}-2   0\end  {array}\right ]$ (red) of length 2 , which is parallel to $x$, therefore $x$ is an eigenvector for $A$. \relax }}{\footnotesize  34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EXERCISES 7.6:}{\footnotesize  37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.7}ITERATIVE METHODS }{\footnotesize  41}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces (a) (left) Plots of the 2-norms of the differences of successive iterates in the Jacobi scheme for the linear system of Example 7.26, using the zero vector as the initial iterate. The convergence is exponential, (b) (right) The corresponding errors when the same scheme is applied to the equivalent linear system with the first two equations being permuted. The sequence now badly diverges, showing the sensitivity of iterative methods to the particular form of the coefficient matrix. \relax }}{\footnotesize  44}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Graph of the number of iterations required for convergence (to a tolerance of $(e-6$ ) using SOR iteration as a function of the relaxation parameter $\omega $. The $k$-values are truncated at 1000 . Notice from the graph that the convergence for GaussSeidel $(\omega =1)$ can be improved. \relax }}{\footnotesize  46}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Comparison of the errors versus the number of iterations for each of the three iteration methods: Jacobi (o), Gauss-Seidel (*), and SOR (x). \relax }}{\footnotesize  47}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Illustration of the maximum absolute value of the eigenvalues of the matrix $I-B^{-1} A$ of Theorem $7.10$ for the SOR method (see Table 7.1) for various values of the relaxation parameter $\omega $. Compare with the corresponding number of iterations needed for convergence (Figure $7.40$ ).\relax }}{\footnotesize  49}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Comparison of the convergence speed of the various iteration methods in the solution of the linear system $A x=b$ of Example $7.30$ where the matrix $A$ is the matrix (63) of size $2500 \times 2500$. In the SOR method the optimal relaxation parameter of Proposition $7.14$ was used. Since we did not invoke any conditioning, the preconditioned conjugate gradient method is simply referred to as the conjugate gradient method. The errors were measured in the infinity norm.\relax }}{\footnotesize  57}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EXERCISES 7.6:}{\footnotesize  59}{}\protected@file@percent }
\gdef \@abspage@last{63}
